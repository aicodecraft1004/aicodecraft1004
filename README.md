# Hi — I’m an AI / Full-Stack Engineer

I build **production-grade AI systems** (LLM features, RAG, and agent workflows) with an emphasis on **evaluation, observability, and reliable delivery**.

- Focus: **measurable quality**, **safe iteration**, and **operational readiness** (not just demos)
- Comfortable owning end-to-end: UX/API/data/integrations → deployment → monitoring
- Best fit: teams shipping AI to real users with constraints (latency, cost, privacy, compliance)

---

## Core capabilities
### LLM + Retrieval (RAG)
- Hybrid retrieval, reranking, metadata filters, citations, freshness strategies
- Guardrails (structured outputs, tool-use constraints, fallback/abstain policies)
- Prompt/version discipline (regression tests, prompt diffs, rollback-ready changes)

### Agents + Workflow Automation
- Tool-using agents with retries/fallbacks/escalation + idempotent actions
- Memory strategies (short-term state, long-term retrieval, structured facts/relationships)
- Human-in-the-loop review flows (confidence gating, audit trails)

### Evaluation + Observability
- Eval harnesses (golden sets, win-rates, regression suites, error taxonomies)
- Telemetry (tracing, cost/latency KPIs, retrieval diagnostics, failure analytics)
- Online monitoring for drift, hallucinations, and retrieval degradation

### Backend + Data (to make AI real)
- Python (FastAPI/Django) / Node.js, Postgres, queues/workers, webhooks
- Integrations (OAuth, third-party APIs), rate limiting, reliability patterns

---

## Selected work (best proof)
- **<Project A: RAG/Agent System>** — <one-line outcome: quality/cost/latency/reliability>  
  Repo: <link> · Demo: <link> · Notes: <link>

- **<Project B: Evaluation/Observability>** — <one-line outcome: regressions caught / quality measured>  
  Repo: <link> · Demo: <link>

- **<Project C: Full-stack AI Feature>** — <one-line outcome: shipped to users / ops reduction>  
  Repo: <link> · Demo: <link>

---

## Mini case study (5 lines)
**Problem:** <what was failing: hallucinations, retrieval misses, inconsistent tool use>  
**Constraints:** <latency/cost/privacy/compliance/timeline>  
**Approach:** <retrieval + evals + guardrails + telemetry>  
**Result:** <metric 1> · <metric 2> · <metric 3>  
**Why it worked:** measurable baselines + instrumentation + iterative fixes

---

## How I work
- Start with a **failure-mode audit** → propose options with tradeoffs
- Establish **eval baselines** before tuning (so changes are measurable)
- Ship incrementally with staging + rollback-friendly releases
- Deliver handoff-ready docs (architecture notes + runbooks + “how to extend”)

---

## Contact (privacy-friendly)
- Discord: **<your_handle>**
- Email: **<work_email_or_alias>**
- Optional: Calendly / Notion intro: <link>

If you share the goal + constraints, I can propose an approach and evaluation plan quickly.
